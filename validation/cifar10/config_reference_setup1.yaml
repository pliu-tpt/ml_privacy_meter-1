run: # Configurations for a specific run
  random_seed: 12345 # Integer number of specifying random seed
  log_dir: test_ccs # String for indicating where to save all the information, including models and computed signals. We can reuse the models saved in the same log_dir.
  time_log: True # Indicate whether to log the time for each step

audit: # Configurations for auditing
  privacy_game: privacy_loss_model # Indicate the privacy game from privacy_loss_model, avg_privacy_loss_training_algo, privacy_loss_sample
  device: cuda:0 # String for indicating on which device we conduct the membership inference attack and train reference models.
  audit_batch_size: 1000 # Integer number for indicating the batch size for computing signals in the privacy meter.
  algorithm: reference # String for indicating the membership inference attack. We currently support population, reference.
  report_log: report_reference # String that indicates the folder where we save the and auditing report.
  num_reference_models: 300 # Integer number for indicating how many reference models we need to train for auditing, if the reference attack is used.
  batch_size: 16 # Integer number for indicating the batch size of training the reference models if reference attack is used.
  f_reference_dataset: 1 # Float number for indicating the ratio between the reference train dataset and the target train dataset.
  optimizer: Adam # String which indicates the optimizer for training reference models. We support Adam and SGD.
  learning_rate: 0.001 # Float number for indicating learning rate for training the reference model.
  weight_decay: 0 # Float number for indicating weight decay for training the reference model.
  model_name: alexnet # String for indicating the model type. We support CNN,alexnet. More model type can be added in model.py.
  epochs: 75 # Integer number for indicating the epochs for training target model.
  split_method: uniform # String for indicating the splitting methods for reference dataset. We support uniform and leave_one_out.
  key: none # String for indicating the meaning of the idx. We support data_idx and none for reusing existing reference models. data_idx search for models which include or exclude the data point indicated using idx. Otherwise, the reference models are the ones that have the same training setting as indicated above.
  idx: none # Integer number for the model index or data index.
  augmentation: None # Indicate whether to use augmentation for the query points

train: # Configuration for training
  type: pytorch # Training framework (we only support pytorch now).
  epochs: 75 # Integer number for indicating the epochs for training target model.
  batch_size: 16 # Integer number for indicating batch size for training the target model.
  test_batch_size: 10000 # Integer number for indicating batch size for evaluating the target model.
  optimizer: Adam # String which indicates the optimizer. We support Adam and SGD.
  learning_rate: 0.001 # Float number for indicating learning rate for training the target model.
  weight_decay: 0 # Float number for indicating weight decay for training the target model.
  model_name: alexnet # String for indicating the model type. We support CNN. More model types can be added in model.py.
  device: cuda:0 # String for indicating the device we want to use for training models.
  key: none # String for indicating the meaning of the idx. We support data_idx, model_idx or none.
  num_target_model: 1 #Integer number for indicating how many target models we want to audit for the privacy game
  test_batch_size: 5000 # Integer number for indicating batch size for evaluating the target model.

data: # Configuration for data
  dataset: cifar10 # String indicates the name of the dataset
  f_train: 0.041666666666666664 # Float number from 0 to 1 indicating the fraction of the train dataset
  f_test: 0.041666666666666664 # Float number from 0 to 1 indicating the fraction of the test dataset
  split_method: no_overlapping # String for indicating the methods of splitting the dataset between train, test, and auditing. Note that, compare to the baseline, we don't use the class balanced splitting.
  f_audit: 0.06666666666666667 # Float from 0 to 1, indicating the fraction of the auditing dataset
  data_dir: ../data
