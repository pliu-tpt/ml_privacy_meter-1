run: # Configurations for a specific run
  random_seed: 12345 # Integer number of specifying random seed
  log_dir: demo_lira_attack # String for indicating where to save all the information, including models and computed signals. We can reuse the models saved in the same log_dir.
  time_log: True # Indicate whether to log the time for each step

audit: # Configurations for auditing
  privacy_game: privacy_loss_model # Indicate the privacy game from privacy_loss_model, avg_privacy_loss_training_algo, privacy_loss_sample
  algorithm: online # String for indicating the membership inference attack. We currently support population, reference.
  report_log: report_online # String that indicates the folder where we save the and auditing report.
  device: cuda:1
  fix_variance: True # Indicate hwether to fix the variance of the signals for all data points
  offline: False # Indicate whether to use the offline setting
  augmentation: None # Indicate whether to use augmentation for the query points
  audit_batch_size: 1000

train: # Configuration for training
  type: pytorch # Training framework (we only support pytorch now).
  model_name: speedyresnet # String for indicating the model type. We support CNN. More model types can be added in model.py.
  device: cuda:1 # String for indicating the device we want to use for training models.
  num_in_models: 8 # Integer number that indicates how many target models trained on idx
  num_out_models: 8 # Integer number that indicates how many target models not trained on idx
  num_target_model: 1 # Number of total models we want to test
  optimizer: speedyresnet_optimizer
  learning_rate: speedyresnet_learning_rate
  weight_decay: speedyresnet_weight_decay
  batch_size: speedyresnet_batch_size
  epochs: speedyresnet_epochs
  test_batch_size: 5000 # Integer number for indicating batch size for evaluating the target model.

data: # Configuration for data
  dataset: cifar10 # String indicates the name of the dataset
  data_dir: ../data
  keep_ratio: 0.5 # the ratio of keeping the point for the training dataset.
  dataset_size: 50000 # the size of the whole dataset used.
